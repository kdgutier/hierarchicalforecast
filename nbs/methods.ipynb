{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierachical Reconciliation Methods\n",
    "\n",
    "Table of Contents\n",
    "1.   [Bottom Up](#cell-1)\n",
    "2.   [Top Down](#cell-2)\n",
    "3.   [Min Trace](#cell-3)\n",
    "4.   [Empirical Risk Minimization](#cell-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "from statsmodels.stats.moment_helpers import cov2corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastcore.test import ExceptionExpected, test_close, test_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _reconcile(S: np.ndarray, P: np.ndarray, W: np.ndarray, \n",
    "               y_hat: np.ndarray, SP: np.ndarray = None):\n",
    "    if SP is None:\n",
    "        SP = S @ P\n",
    "    return np.matmul(SP, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-1\"></a>\n",
    "## 1. Bottom Up\n",
    "\n",
    "The most basic hierarchical reconciliation is performed using an Bottom-Up strategy. It was proposed for the first time by Orcutt in 1968.\n",
    "The corresponding hierarchical \"projection\" matrix is defined as:\n",
    "\n",
    "$$\\mathbf{P}_{\\text{BU}} = [\\mathbf{0}_{\\mathrm{[b],[a]}}\\;|\\;\\mathbf{I}_{\\mathrm{[b][b]}}]$$\n",
    "\n",
    "- [Orcutt, G.H., Watts, H.W., & Edwards, J.B.(1968). Data aggregation and information loss. The American Economic Review, 58 , 773{787)](http://www.jstor.org/stable/1815532)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def bottom_up(S: np.ndarray,\n",
    "              y_hat: np.ndarray,\n",
    "              idx_bottom: List[int]):\n",
    "    n_hiers, n_bottom = S.shape\n",
    "    P = np.zeros_like(S, dtype=np.float32)\n",
    "    P[idx_bottom] = S[idx_bottom]\n",
    "    P = P.T\n",
    "    W = np.eye(n_hiers, dtype=np.float32)\n",
    "    return _reconcile(S, P, W, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BottomUp:\n",
    "    \n",
    "    def reconcile(self,\n",
    "                  S: np.ndarray,\n",
    "                  y_hat: np.ndarray,\n",
    "                  idx_bottom: np.ndarray):\n",
    "        return bottom_up(S=S, y_hat=y_hat, idx_bottom=idx_bottom)\n",
    "    \n",
    "    __call__ = reconcile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "S = np.array([\n",
    "    [1., 1., 1., 1.],\n",
    "    [1., 1., 0., 0.],\n",
    "    [0., 0., 1., 1.],\n",
    "    [0., 1., 0., 0.],\n",
    "    [1., 0., 0., 0.],\n",
    "    [0., 0., 1., 0.],\n",
    "    [0., 0., 0., 1.],\n",
    "])\n",
    "h = 10\n",
    "_y = np.array([10., 5., 4., 2., 1.])\n",
    "y_bottom = np.vstack([i * _y for i in range(1, 5)])\n",
    "resids_bottom = y_bottom - np.roll(y_bottom, 1)\n",
    "resids_bottom[:, 0] = np.nan\n",
    "y_hat_bottom = np.vstack([i * np.ones(h) for i in range(1, 5)])\n",
    "idx_bottom = [4, 3, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "cls_bottom_up = BottomUp()\n",
    "test_eq(\n",
    "    cls_bottom_up(S=S, y_hat=S @ y_hat_bottom, idx_bottom=idx_bottom),\n",
    "    S @ y_hat_bottom\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-2\"></a>\n",
    "## 2. Top Down\n",
    "\n",
    "The Top Down hierarchical reconciliation method, distributes the total aggregate predictions and decomposes it down the hierarchy using proportions $\\mathbf{p}_{\\mathrm{[b]}}$ that can be actual historical values or estimated.\n",
    "\n",
    "$$\\mathbf{P}=[\\mathbf{p}_{\\mathrm{[b]}}\\;|\\;\\mathbf{0}_{\\mathrm{[b][a,b\\;-1]}}]$$\n",
    "\n",
    "- [Disaggregation methods to expedite product line forecasting. Journal of Forecasting, 9 , 233–254. doi:10.1002/for.3980090304](https://onlinelibrary.wiley.com/doi/abs/10.1002/for.3980090304)\n",
    "- [An investigation of aggregate variable time series forecast strategies with specific subaggregate time series statistical correlation. Computers and Operations Research, 26 , 1133–1149. doi:10.1016/S0305-0548(99)00017-9](https://doi.org/10.1016/S0305-0548(99)00017-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def top_down(S: np.ndarray, \n",
    "             y_hat: np.ndarray,\n",
    "             y: np.ndarray,\n",
    "             idx_bottom: List[int],\n",
    "             method: str):\n",
    "    n_hiers, n_bottom = S.shape\n",
    "    idx_top = int(S.sum(axis=1).argmax())\n",
    "    #add strictly hierarchical assert\n",
    "    \n",
    "    if method == 'forecast_proportions':\n",
    "        raise NotImplementedError(f'Method {method} not implemented yet')\n",
    "    else:\n",
    "        y_top = y[idx_top]\n",
    "        y_btm = y[idx_bottom]\n",
    "        if method == 'average_proportions':\n",
    "            prop = np.mean(y_btm / y_top, axis=1)\n",
    "        elif method == 'proportion_averages':\n",
    "            prop = np.mean(y_btm, axis=1) / np.mean(y_top)\n",
    "        else:\n",
    "            raise Exception(f'Unknown method {method}')\n",
    "    P = np.zeros_like(S).T\n",
    "    P[:, idx_top] = prop\n",
    "    W = np.eye(n_hiers, dtype=np.float32)\n",
    "    return _reconcile(S, P, W, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TopDown:\n",
    "    \n",
    "    def __init__(self, method: str):\n",
    "        self.method = method\n",
    "    \n",
    "    def reconcile(self, \n",
    "                  S: np.ndarray, \n",
    "                  y_hat: np.ndarray,\n",
    "                  y: np.ndarray,\n",
    "                  idx_bottom: List[int]):\n",
    "        return top_down(S=S, y_hat=y_hat, y=y, \n",
    "                        idx_bottom=idx_bottom,\n",
    "                        method=self.method)\n",
    "    \n",
    "    __call__ = reconcile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "for method in ['average_proportions', 'proportion_averages']:\n",
    "    cls_top_down = TopDown(method=method)\n",
    "    test_close(\n",
    "        cls_top_down(\n",
    "            S=S, \n",
    "            y_hat=S @ y_hat_bottom, \n",
    "            y=S @ y_bottom, \n",
    "            idx_bottom=idx_bottom\n",
    "        ),\n",
    "        S @ y_hat_bottom\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-3\"></a>\n",
    "## 3. Min Trace\n",
    "\n",
    "This reconciliation algorithm proposed by Wickramasuriya et al. depends on a generalized least squares estimator and an estimator of the covariance matrix of the coherency errors $\\mathbf{W}_{h}$. The Min Trace algorithm minimizes the squared errors for the coherent forecasts under an unbiasedness assumption; the solution has a closed form.\n",
    "\n",
    "$$\\mathbf{P}_{\\text{MinT}} = \\left(\\mathbf{S}^{\\intercal}\\mathbf{W}_{h}\\mathbf{S}\\right)^{-1} \\mathbf{S}^{\\intercal} \\mathbf{W}^{-1}_{h}$$\n",
    "\n",
    "- [Wickramasuriya, S. L., Athanasopoulos, G., & Hyndman, R. J. (2019). Optimal forecast reconciliation for hierarchical and grouped time series through trace minimization. Journal of the American Statistical Association, 114 , 804–819. doi:10.1080/01621459.2018.1448825.](https://robjhyndman.com/publications/mint/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def crossprod(x):\n",
    "    return x.T @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def min_trace(S: np.ndarray, \n",
    "              y_hat: np.ndarray,\n",
    "              residuals: np.ndarray,\n",
    "              method: str):\n",
    "    # shape residuals (obs, n_hiers)\n",
    "    res_methods = ['wls_var', 'mint_cov', 'mint_shrink']\n",
    "    if method in res_methods and residuals is None:\n",
    "        raise ValueError(f\"For methods {', '.join(res_methods)} you need to pass residuals\")\n",
    "    n_hiers, n_bottom = S.shape\n",
    "    if method == 'ols':\n",
    "        W = np.eye(n_hiers)\n",
    "    elif method == 'wls_struct':\n",
    "        W = np.diag(S @ np.ones((n_bottom,)))\n",
    "    elif method in res_methods:\n",
    "        n, _ = residuals.shape\n",
    "        masked_res = np.ma.array(residuals, mask=np.isnan(residuals))\n",
    "        covm = np.ma.cov(masked_res, rowvar=False, allow_masked=True).data\n",
    "        if method == 'wls_var':\n",
    "            W = np.diag(np.diag(covm))\n",
    "        elif method == 'mint_cov':\n",
    "            W = covm\n",
    "        elif method == 'mint_shrink':\n",
    "            tar = np.diag(np.diag(covm))\n",
    "            corm = cov2corr(covm)\n",
    "            xs = np.divide(residuals, np.sqrt(np.diag(covm)))\n",
    "            xs = xs[~np.isnan(xs).any(axis=1), :]\n",
    "            v = (1 / (n * (n - 1))) * (crossprod(xs ** 2) - (1 / n) * (crossprod(xs) ** 2))\n",
    "            np.fill_diagonal(v, 0)\n",
    "            corapn = cov2corr(tar)\n",
    "            d = (corm - corapn) ** 2\n",
    "            lmd = v.sum() / d.sum()\n",
    "            lmd = max(min(lmd, 1), 0)\n",
    "            W = lmd * tar + (1 - lmd) * covm\n",
    "    else:\n",
    "        raise ValueError(f'Unkown reconciliation method {method}')\n",
    "    \n",
    "    eigenvalues, _ = np.linalg.eig(W)\n",
    "    if any(eigenvalues < 1e-8):\n",
    "        raise Exception(f'min_trace ({method}) needs covariance matrix to be positive definite.')\n",
    "        \n",
    "    R = S.T @ np.linalg.inv(W)\n",
    "    P = np.linalg.inv(R @ S) @ R\n",
    "    \n",
    "    return _reconcile(S, P, W, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MinTrace:\n",
    "    \n",
    "    def __init__(self, method: str):\n",
    "        self.method = method\n",
    "        \n",
    "    def reconcile(self, \n",
    "                  S: np.ndarray, \n",
    "                  y_hat: np.ndarray,\n",
    "                  residuals: np.ndarray):\n",
    "        return min_trace(S=S, y_hat=y_hat, \n",
    "                         residuals=residuals,\n",
    "                         method=self.method)\n",
    "    \n",
    "    __call__ = reconcile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "for method in ['ols', 'wls_struct', 'wls_var', 'mint_shrink']:\n",
    "    cls_min_trace = MinTrace(method=method)\n",
    "    test_close(\n",
    "        cls_min_trace(\n",
    "            S, \n",
    "            S @ y_hat_bottom, \n",
    "            np.transpose(S @ resids_bottom)\n",
    "        ),\n",
    "        S @ y_hat_bottom\n",
    "    )\n",
    "with ExceptionExpected(regex='min_trace (mint_cov)*'):\n",
    "    cls_min_trace = MinTrace(method='mint_cov')\n",
    "    cls_min_trace(\n",
    "        S, \n",
    "        S @ y_hat_bottom, \n",
    "        np.transpose(S @ resids_bottom)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def erm(S: np.ndarray,\n",
    "        y_hat: np.ndarray,\n",
    "        method: str,\n",
    "        lambda_reg: float = 1e-2):\n",
    "    n_hiers, n_bottom = S.shape\n",
    "    if method == 'exact':\n",
    "        B = y_hat.T @ S @ np.linalg.inv(S.T @ S).T\n",
    "        P = B.T @ y_hat.T @ np.linalg.inv(y_hat @ y_hat.T + lambda_reg * np.eye(n_hiers))\n",
    "    else:\n",
    "        raise ValueError(f'Unkown reconciliation method {method}')\n",
    "        \n",
    "    W = np.eye(n_hiers, dtype=np.float32)\n",
    "    \n",
    "    return _reconcile(S, P, W, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cell-4\"></a>\n",
    "## 4. Empirical Risk Minimization\n",
    "\n",
    "The Empirical Risk Minimization reconciliation strategy relaxes the unbiasedness assumptions from\n",
    "previous reconciliation methods like MinT and optimizes square errors between the reconciled predictions\n",
    "and the validation data to obtain an optimal reconciliation matrix P.\n",
    "\n",
    "The exact solution for $\\mathbf{P}$ (`method='exact'`) follows the expression:\n",
    "\n",
    "$$\\mathbf{P}^{*} = \\left(\\mathbf{S}^{\\intercal}\\mathbf{S}\\right)^{-1}\\mathbf{Y}^{\\intercal}\\hat{\\mathbf{Y}}\\left(\\hat{\\mathbf{Y}}\\hat{\\mathbf{Y}}\\right)^{-1}$$\n",
    "\n",
    "The alternative Lasso regularized $\\mathbf{P}$ solution (`method='lasso'`) is useful when the observations of validation data is \n",
    "limited or the exact solution has low numerical stability.\n",
    "\n",
    "$$\\mathbf{P}^{*} = \\text{argmin}_{\\mathbf{P}} ||\\mathbf{Y}-\\mathbf{S} \\mathbf{P} \\hat{Y} ||^{2}_{2} + \\lambda ||\\mathbf{P}-\\mathbf{P}_{\\text{BU}}||_{1}$$\n",
    "\n",
    "\n",
    "\n",
    "- [Ben Taieb, S., & Koo, B. (2019). Regularized regression for hierarchical forecasting without unbiasedness conditions. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining KDD '19 (p. 1337{1347). New York, NY, USA: Association for Computing Machinery.](https://doi.org/10.1145/3292500.3330976)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _solve(a, b, c):\n",
    "    \"\"\"Return solution of a quadratic minimization.\n",
    "    The optimization equation is:\n",
    "         f(a, b, c) = argmin_w{1/2 * a * w^2 + b * w + c * |w|}\n",
    "    we get optimal solution w*:\n",
    "         w* = -(b - sign(b)*c)/a if |b| > c else w* = 0\n",
    "    REQUIRES: Dimensionality of a and b must be same\n",
    "    Args:\n",
    "      a: array\n",
    "      b: array\n",
    "      c: array with one element.\n",
    "    Returns:\n",
    "      A Tensor w, which is solution for the equation\n",
    "    \"\"\"\n",
    "    w = (c * np.sign(b) - b) / a\n",
    "    w = (np.abs(b) > c) * w\n",
    "    return w\n",
    "\n",
    "class AdamOptimizer:\n",
    "    \"\"\"Stochastic gradient descent optimizer with Adam\n",
    "    \n",
    "    Note: All default values are from the original Adam paper\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        params : list, length = len(coefs_) + len(intercepts_)\n",
    "            The concatenated list containing coefs_ and intercepts_ in MLP model.\n",
    "            Used for initializing velocities and updating params\n",
    "        learning_rate_init : float, default=0.001\n",
    "            The initial learning rate used. It controls the step-size in updating\n",
    "            the weights\n",
    "        beta_1 : float, default=0.9\n",
    "            Exponential decay rate for estimates of first moment vector, should be\n",
    "            in [0, 1)\n",
    "        beta_2 : float, default=0.999\n",
    "            Exponential decay rate for estimates of second moment vector, should be\n",
    "            in [0, 1)\n",
    "        epsilon : float, default=1e-8\n",
    "            Value for numerical stability\n",
    "            \n",
    "        Attributes\n",
    "        ----------\n",
    "        learning_rate : float\n",
    "            The current learning rate\n",
    "        t : int\n",
    "            Timestep\n",
    "        ms : list, length = len(params)\n",
    "            First moment vectors\n",
    "        vs : list, length = len(params)\n",
    "            Second moment vectors\n",
    "            \n",
    "        References\n",
    "        ----------\n",
    "        [1] Kingma, Diederik, and Jimmy Ba (2014) \"Adam: A method for\n",
    "            stochastic optimization.\"\n",
    "\n",
    "        [2] Sashank J. Reddi, Satyen Kale, Sanjiv Kumar (2019). \"On the \n",
    "            Convergence of Adam and Beyond.\". \n",
    "            https://arxiv.org/abs/1904.09237v1\n",
    "        \n",
    "        [3] Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, Sanjiv Kumar (2019).\n",
    "            \"Adaptive Methods for Nonconvex Optimization\".\n",
    "            https://papers.nips.cc/paper/2018/hash/90365351ccc7437a1309dc64e4db32a3-Abstract.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, params, learning_rate_init=0.001, \n",
    "        beta_1=0.9, beta_2=0.999, epsilon=1e-8,\n",
    "        l1_reg=0,\n",
    "    ):        \n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.learning_rate = float(learning_rate_init)        \n",
    "\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.l1_reg = l1_reg\n",
    "        \n",
    "        self.t = 0\n",
    "        self.ms = [np.zeros_like(param) for param in params]\n",
    "        self.vs = [np.zeros_like(param) for param in params]\n",
    "        \n",
    "    def update_params(self, params, grads):\n",
    "        \"\"\"Update parameters with given gradients\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : list of length = len(coefs_) + len(intercepts_)\n",
    "            The concatenated list containing coefs_ and intercepts_ in MLP\n",
    "            model. Used for initializing velocities and updating params\n",
    "        grads : list of length = len(params)\n",
    "            Containing gradients with respect to coefs_ and intercepts_ in MLP\n",
    "            model. So length should be aligned with params\n",
    "        \"\"\"\n",
    "        updates = self._get_updates(grads)\n",
    "        for param, update, per_coord_lr in zip((p for p in params), updates, \n",
    "                                               self.per_coord_lrs):\n",
    "            param += update\n",
    "            if self.l1_reg>0:\n",
    "                param = _solve(1, -param, self.l1_reg * per_coord_lr)\n",
    "        return params  \n",
    "\n",
    "    def _get_updates(self, grads):\n",
    "        \"\"\"Get the values used to update params with given gradients\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : list, length = len(coefs_) + len(intercepts_)\n",
    "            Containing gradients with respect to coefs_ and intercepts_ in MLP\n",
    "            model. So length should be aligned with params\n",
    "        Returns\n",
    "        -------\n",
    "        updates : list, length = len(grads)\n",
    "            The values to add to params\n",
    "        \"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        # Yogi, correction\n",
    "        signs = np.sign(grads**2 - self.vs)\n",
    "        \n",
    "        self.ms = [\n",
    "            self.beta_1 * m + (1 - self.beta_1) * grad\n",
    "            for m, grad in zip(self.ms, grads)\n",
    "        ]\n",
    "        self.vs = [\n",
    "            #np.maximum(v, self.beta_2 * v + (1 - self.beta_2) * (grad**2))\n",
    "            v + (1 - self.beta_2) * sign * (grad**2)\n",
    "            for v, grad, sign in zip(self.vs, grads, signs)\n",
    "        ]\n",
    "        \n",
    "        self.per_coord_lrs = [\n",
    "            self.learning_rate_init * \n",
    "            (np.sqrt(1 - self.beta_2**self.t) / (1 - self.beta_1**self.t)) /\n",
    "            (np.sqrt(v) + self.epsilon)\n",
    "            for v in self.vs\n",
    "        ]\n",
    "        \n",
    "        updates = [ -lr * m for m, lr in zip(self.ms, self.per_coord_lrs)]\n",
    "        \n",
    "        return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ERM:\n",
    "    \n",
    "    def __init__(self, method: str, \n",
    "                 y_valid: np.array, y_valid_hat: np.array,\n",
    "                 l1_reg: int = 300, learning_rate: float = 1e-1, batch_size: int = 1, \n",
    "                 max_iters: int = 6_000, init_zeros: bool = False, random_state: int = 0):\n",
    "        \n",
    "        assert method in ['exact', 'lasso'], f'Unkown reconciliation method {method}'\n",
    "        self.method = method\n",
    "\n",
    "        # Lasso hyperparameters\n",
    "        self.l1_reg = l1_reg\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iters = max_iters\n",
    "        \n",
    "        self.init_zeros = init_zeros\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.y_valid = y_valid\n",
    "        self.y_valid_hat = y_valid_hat\n",
    "\n",
    "    def _exact_erm_matrix(self, S: np.array, Y: np.array, Y_hat: np.array):\n",
    "        \"\"\"\n",
    "        The quality of this closed form solution of the projection matrix depends on having \n",
    "        large enough of uncorrelated Y_hat values for it to have a good condition number.\n",
    "        We offer the alternative of the Frobeniuous optimized alternative.\n",
    "        \"\"\"\n",
    "        n_total, n_bottom = S.shape\n",
    "        temp1 = np.linalg.inv(S.T @ S + 1e-2*np.eye(n_bottom))\n",
    "        temp2 = (S.T @ Y) @ Y_hat.T\n",
    "        temp3 = np.linalg.inv(Y_hat @ Y_hat.T + 1e-1*np.eye(n_total))\n",
    "        P     = (temp1 @ temp2) @ temp3\n",
    "        \n",
    "        # TODO: Write warning on the condition number.\n",
    "        # print(\"cond_number(temp1)\", np.linalg.cond(temp1))\n",
    "        # print(\"cond_number(temp3)\", np.linalg.cond(temp3))\n",
    "        return P\n",
    "\n",
    "    def _compute_loss_and_gradients(self, S, P, Pbu, y, y_hat):\n",
    "        # Wrangling data and flatten it\n",
    "        # [n_batch,n_total,horizon] -> [n_total,n_batch,horizon]\n",
    "        n_batch,n_total,horizon = y.shape\n",
    "        y_t = np.transpose(y, (1, 0, 2))\n",
    "        y_hat_t = np.transpose(y_hat, (1, 0, 2))\n",
    "        y_t = y_t.reshape(n_total,n_batch*horizon)\n",
    "        y_hat_t = y_hat_t.reshape(n_total,n_batch*horizon)\n",
    "        y_prime = S @ P @ y_hat_t\n",
    "        \n",
    "        # Compute loss\n",
    "        main_loss  = np.mean((y_t - y_prime)**2)\n",
    "        reg_loss   = np.mean(np.abs(P - Pbu))\n",
    "        loss = main_loss + self.l1_reg * reg_loss\n",
    "        \n",
    "        # Compute gradients \n",
    "        g1 = (2 * S.T @ y_prime - 2 * S.T @ y_t) @ y_hat_t.T\n",
    "        g2 = self.l1_reg * np.sign(P-Pbu)\n",
    "        \n",
    "        grads = g1 + g2     \n",
    "        \n",
    "        return loss, grads\n",
    "\n",
    "    def _lasso_erm_matrix(self, S: np.array, Y: np.array, Y_hat: np.array):\n",
    "        \"\"\"\n",
    "        This version of the projection matrix optimizes the reconciliation reconstruction\n",
    "        error, with Frobenius L1 regularization of the projection around the bottom-up matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Parse hyperparameters and prepare data\n",
    "        n_total, n_bottom = S.shape\n",
    "        n_agg = n_total - n_bottom\n",
    "        \n",
    "        if len(Y.shape)==2:\n",
    "            Y = Y[None,:,:]\n",
    "            Y_hat = Y_hat[None,:,:]\n",
    "        \n",
    "        # Initialize P matrix\n",
    "        Pbu = np.concatenate([np.zeros((n_agg,n_bottom)), \n",
    "                              np.eye(n_bottom)], axis=0).T\n",
    "        if self.init_zeros:\n",
    "            P = np.zeros(Pbu.shape)\n",
    "        else:\n",
    "            P = Pbu.copy()\n",
    "        \n",
    "        optimizer = AdamOptimizer(params=P,\n",
    "                                  beta_1=0.9,\n",
    "                                  beta_2=0.999,\n",
    "                                  epsilon=1e-8,\n",
    "                                  learning_rate_init=self.learning_rate,\n",
    "                                  l1_reg=self.l1_reg)\n",
    "        \n",
    "        # Datasets permutation for stochastic optimization\n",
    "        # numpy random seed for replicability, and data augmentation\n",
    "        np.random.seed(self.random_state)\n",
    "        batch_idxs = np.random.permutation(len(Y)) \n",
    "        Y = Y[batch_idxs,:,:]\n",
    "        Y_hat = Y_hat[batch_idxs,:,:]\n",
    "        \n",
    "        idx = 0\n",
    "        prev_loss = 0.0\n",
    "        batch_size = self.batch_size\n",
    "        iterator = tqdm(range(self.max_iters), mininterval=1)\n",
    "        for step in iterator:            \n",
    "            \n",
    "            # Stochastic gradient's sample\n",
    "            batch_y = Y[idx:idx+batch_size,:,:]\n",
    "            batch_y_hat = Y_hat[idx:idx+batch_size,:,:]\n",
    "            idx = (idx + batch_size) % len(Y)\n",
    "            \n",
    "            loss, grads = self._compute_loss_and_gradients(S=S, P=P, Pbu=Pbu, \n",
    "                                                           y=batch_y, y_hat=batch_y_hat)\n",
    "            P = optimizer.update_params(params=P, grads=grads)\n",
    "            iterator.set_description(f'ERM Loss = {np.round(loss,4)}')\n",
    "            prev_loss = loss\n",
    "            \n",
    "        return P\n",
    "        \n",
    "    def reconcile(self, S: np.ndarray,\n",
    "                  y_hat: np.ndarray):\n",
    "\n",
    "        if self.method=='exact':\n",
    "            self.P = self._exact_erm_matrix(S=S, Y=self.y_valid, Y_hat=self.y_valid_hat)\n",
    "        elif self.method=='lasso':\n",
    "            self.P = self._lasso_erm_matrix(S=S, Y=self.y_valid, Y_hat=self.y_valid_hat)\n",
    "\n",
    "        y_hat_reconc = S @ self.P @ y_hat\n",
    "\n",
    "        return y_hat_reconc\n",
    "    \n",
    "    __call__ = reconcile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
